#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Monday
Agenda for today:
Exercise time for cd ../Evaluation
Walkthrough
Modeling!!

## Notes 
- stratfy on the variable you want to predict (sklearn(t,v,test)

- x_train = all vatiables 
- y_train = target variable (thing we want to predict)

- To predict proablity use 
- clf.predict_praba(x_train)

- evalute the model by doing 
- (train.smoker == train.baseline).mean()

- make sure after you make a model on internal data, train it with outside data.

- be sure to set max_depth = 3 so you dont overfit

- try to make train accuracy and validate acuarcy around the same

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Tuesday 

## Notes 
Random Forest is a type of Ensemble Machine Learning algo called Bootsrap Aggregation of bagging

model type= Random Forest
Random Forest is a kind of Ensemble ML application 
Any  time we combine multiple algos together == Ensemble

short version = we make a whole bunch of treees and then average the predictions

If we have a gigantic population of observations it may be time/cost
prohibative to meausre every one of those observations.

Bootstraping = taking a bunch of smaples, then averageing the mean for each samply, 
and then avg those sample averages (only if the sampkle size is the same)

Dependent on the central limit therom

### The more samples you have the closer you will get to the population mean 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Wendsday

## Notes
What is KNN?

    Supervised Algorithm

    Makes predictions based on how close a new data point is to known data points.

    Considered a lazy algorithm in that it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the k nearest neighbours of each point.

    Predictions are made for a new data point by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression problems, this might be the mean output variable. For classification problems this might be the mode (or most common) class value.

    It is important to define a metric to measure how similar data instances are. Euclidean distance can be used if attributes are all on the same scale (or you convert them to the same scale).

